# Run init_genome.snakes before running this Snakefile
# This Snakefile handles dataset-specific analysis, assuming that reference genome
# file dependencies and file of output filenames have been made using init_genome.snakes

import os, re
import pandas as pd
from Bio import SeqIO
shell.executable("bash")

configfile: "config.yaml"

units = pd.read_csv(config["units"], index_col=["sample", "unit"], dtype=str, sep = "\t")
units.index = units.index.set_levels([i.astype(str) for i in units.index.levels])

# need to build dictionary where key is sample and values are all unit files associated with that sample
# try doing this from pandas?

samples = {}
for i in units.index.levels[0]:
    samples[i] = ["data/clipOverlap/{}-{}.bam".format(i, j) for j in units.loc[i].index]

# todo fix, accomodating shell-based off by 1 error
# scaffolds = [line.rstrip('\n') for line in open("data/intervals/scaffold_intervals.txt", 'r')]

def is_single_end(sample,unit):
    return pd.isnull(units.loc[(sample, unit), "fq2"])

def get_fastq(wildcards):
    return "data/reads/"+units.loc[(wildcards.sample, wildcards.unit), ["fq1", "fq2"]].dropna()

def get_trimmed(wildcards):
    if not is_single_end(**wildcards):
        return expand("data/trimmed/{sample}-{unit}-{group}.fastq.gz",
            group=[1,2], **wildcards)
    return "data/trimmed/{sample}-{unit}.fastq.gz".format(**wildcards)
    
def get_intervals(ref, main_size, backup_size=3000):
    
    """
    Identify natural breaks in genome assembly for scatter-gather variant calling
    """
    
    intervals = []

    with open(ref, 'r') as handle:
        for record in SeqIO.parse(handle, "fasta"):
            
            if record.id not in config["good_chroms"]:
                continue
            
            print("Finding gaps on {}".format(record.id))
            
            start = 0
            tmp_int = []
            s=record.seq
            scaff_regex = "N"*int(main_size)+"+"
            intervals += chunk_by_gap(record, main_size, backup_size)
        
    o = open(config["intervals"], 'w')
    o.write('\n'.join(intervals)+'\n')
    return intervals
    
def chunk_by_gap(record, main_size, backup_size=3000, hardcut_size=1e6):

    """
    Identify gaps in one SeqIO fasta entry
    If dividing by gaps of main_size fails, attempt to divide by backup_size gaps
    """
    
    start = 0
    tmp_int = []
    scaff_regex = "N{" + str(int(main_size)) + ",}"
    # print(scaff_regex)
    
    for match in re.finditer(scaff_regex, str(record.seq)):
        tmp_int.append("{}\t{}\t{}".format(record.id, start, match.start()))
        start = match.end()
    
    # Was it divided up? If not, try a smaller gap size
    if start == 0:
        backup_regex = "N{" + str(int(backup_size)) + ",}"
        for match in re.finditer(backup_regex, str(record.seq)):
            tmp_int.append("{}\t{}\t{}".format(record.id, start, match.start()))
            start = match.end()
    
    # Handle last interval if fasta doesn't end with Ns
    if start != 0 and start < len(record):
        tmp_int.append("{}\t{}\t{}".format(record.id, start, len(record)))
        
        
    # Handle last interval if fasta ends with Ns
    elif start > len(record):
        fix_start = tmp_int[-1].split('\t')[1]
        tmp_int[-1] = ("{}\t{}\t{}".format(record.id, fix_start, len(record)))
        
    # if still haven't made a cut, do a hard cut
    elif start == 0:
        while start < len(record)-hardcut_size:
            tmp_int.append("{}\t{}\t{}".format(record.id, int(start), int(start)+int(hardcut_size)))
            start += hardcut_size
        tmp_int.append("{}\t{}\t{}".format(record.id, int(start), len(record)))
    
    return tmp_int

try:
    ifh = open(config["intervals"], 'r')
    intervals = []
    for line in ifh:
        intervals.append(line.rstrip())
except FileNotFoundError:
    intervals = get_intervals(config["genome"], 5e4)

rule all:
    input:
        ["data/realigned/{}.bam".format(x) for x in units.index.levels[0]],
        ["data/depths/{}_q20_depth.bed".format(x) for x in units.index.levels[0]],
        "data/calls/MM-calls.vcf",
        ["data/calls/{}-MM-filtered-calls.vcf.gz".format(x) for x in config["good_chroms"]],
        expand(["data/parsed_calls/{mom}-{pop}-calls.vcf.gz"], mom=config["moms"], pop=config["pops"])

# include: "rules/intersect_offchrom.rules"
# include: "rules/vcf_site_filters.rules"
# include: "rules/merge_MM_vcfs_to_chrom.rules"
# include: "rules/merge_MM_vcfs.rules"
# include: "rules/freebayes_interval_MM.rules"
# include: "rules/samtools_q20_depth.rules"
# include: "rules/samtools_q0_depth.rules"
# include: "rules/realign_indels.rules" # testing placement at this point in the workflow, rule rewritten
# include: "rules/realigner_target_creator.rules" # testing placement at this point in the workflow, rule rewritten
# include: "rules/samtools_index_pe.rules" # change for later indel realign
# include: "rules/samtools_merge.rules" # change for later indel realign
# include: "rules/clip_overlap.rules" # changed for later indel realign
# include: "rules/filter_good_pairs.rules"
# include: "rules/mark_duplicates.rules"
# include: "rules/align.rules"
# include: "rules/cutadapt_pe.rules"
# include: "rules/cutadapt.rules"

rule cutadapt:
    input:
        get_fastq
    output:
        fastq="data/trimmed/{sample}-{unit}.fastq.gz",
        qc="data/trimmed/{sample}-{unit}.qc.txt"
    threads: config["params"]["cutadapt-se"]["threads"]
    params:
        "-a {} {}".format(config["adapter"], config["params"]["cutadapt-se"]["qual"])
    log:
        "logs/cutadapt/{sample}-{unit}.log"
    conda:
        "../envs/cutadapt.yaml"
    wrapper:
        "0.17.4/bio/cutadapt/se"
        
rule cutadapt_pe:
    input:
        get_fastq
    output:
        fastq1="data/trimmed/{sample}-{unit}-1.fastq.gz",
        fastq2="data/trimmed/{sample}-{unit}-2.fastq.gz",
        qc="data/trimmed/{sample}-{unit}.qc.txt"
    threads: config["params"]["cutadapt-pe"]["threads"]
    params:
        "-a {} -A {} {}".format(config["adapter"], config["adapter"], config["params"]["cutadapt-pe"]["qual"])
    log:
        "logs/cutadapt/{sample}-{unit}.log"
    conda:
        "../envs/cutadapt.yaml"
    wrapper:
        "0.17.4/bio/cutadapt/pe"
        
rule align:
    input:
        reads=get_trimmed,
        ref=config["genome"],
        index=config["genome"]+'.bwt'
    output:
        "data/aligned_reads/{sample}-{unit}.bam"
    log:
        "logs/bwa_mem/{sample}-{unit}.log"
    conda:
        "../envs/bwa-samtools.yaml"
    threads:
        config["params"]["align"]["threads"]
    params:
        rg="'@RG\\tID:{unit}\\tSM:{sample}'",
        bwa_threads=3*config["params"]["align"]["threads"] // 4,
        sort_threads=config["params"]["align"]["threads"] // 4,
        sort_mem=config["params"]["align"]["sort_mem"]
    shell: """
        bwa mem -R {params.rg} -t {params.bwa_threads} {input.ref} {input.reads} 2> {log} | \
        samtools sort -@{params.sort_threads} -m {params.sort_mem} -o {output} -
    """

rule mark_duplicates:
    input:
        "data/aligned_reads/{sample}-{unit}.bam"
    output:
        bam="data/dedup/{sample}-{unit}.bam",
        metrics="data/dedup/{sample}-{unit}-metrics.txt"
    log:
        "logs/picard/{sample}-{unit}.log"
    conda:
        "../envs/picard.yaml"
    params:
        jarpath=config["params"]["mark_duplicates"]["jarpath"],
        java_heap=config["params"]["mark_duplicates"]["java_heap"],
        opt=config["params"]["mark_duplicates"]["opt"]
    shell:
        "java {params.java_heap} -jar {params.jarpath} MarkDuplicates INPUT={input} "
        "OUTPUT={output.bam} METRICS_FILE={output.metrics} {params.opt} 2> {log}"

rule filter_good_pairs:
    input:
        "data/dedup/{sample}-{unit}.bam"
    output:
        "data/pairfilt/{sample}-{unit}.bam"
    conda:
        "../envs/bwa-samtools.yaml"
    shell: """
        samtools view -h {input} | awk '{{if ($7 == "=" || $7 == "*" || $1 ~ /@[A-Za-z]{{2}}/) print $0}}' | \
            samtools view -b -o {output} -
        """

rule clip_overlap:
    input:
        # "data/realigned/{sample}-{unit}.bam"
        "data/pairfilt/{sample}-{unit}.bam"
    output:
        "data/clipOverlap/{sample}-{unit}.bam"
    log:
        "logs/clipOverlap/{sample}-{unit}.log"
    conda:
        "../envs/bamutil.yaml"
    shell: """
        bam clipOverlap --in {input} --out {output} --stats --unmapped 2> {log}
    """
    
rule samtools_merge:
    input:
        lambda x: samples[x.sample] # this accesses all files and wildcards properly
    output:
        "data/merged/{sample}.bam"
    conda:
        "../envs/bwa-samtools.yaml"
    shell:
        "samtools merge {output} {input}"
        
rule samtools_index_pe:
    input:
        "data/merged/{sample}.bam"
    output:
        "data/merged/{sample}.bam.bai"
    conda:
        "../envs/bwa-samtools.yaml"
    shell:
        "samtools index {input}"

rule realigner_target_creator:
    input:
        ref=config["genome"],
        dict=re.sub("(\.fasta$|\.fa$)", ".dict", config["genome"]),
        # bam="data/pairfilt/{sample}-{unit}.bam" 
        bam="data/merged/{sample}.bam"
    output:
        # "data/intervals/{sample}-{unit}.intervals"
        "data/intervals/{sample}.intervals"
    params:
        jvm=config["params"]["gatk"]["jvm"],
        jar=config["params"]["gatk"]["jar"]
    threads: config["params"]["gatk"]["threads"]
    log:
        "logs/gatk3/realigntarget-{sample}.log"
    conda:
        "../envs/bwa-samtools.yaml"
    shell: """
         samtools index {input.bam}
         java {params.jvm} -jar {params.jar} \
             -T RealignerTargetCreator \
             -R {input.ref} \
             -I {input.bam} \
             -o {output} \
             2> {log}
    """

rule realign_indels:
    input:
        ref=config["genome"],
        dict=re.sub("(\.fasta$|\.fa$)", ".dict", config["genome"]),
        #bam="data/pairfilt/{sample}-{unit}.bam",
        #intervals="data/intervals/{sample}-{unit}.intervals"
        bam="data/merged/{sample}.bam",
        intervals="data/intervals/{sample}.intervals"
    output:
        #"data/realigned/{sample}-{unit}.bam"
        "data/realigned/{sample}.bam",
        "data/realigned/{sample}.bam.bai"
    params:
        jvm=config['params']['gatk']['jvm'],
        jar=config['params']['gatk']['jar']
    log:
        # "logs/realigned/{sample}-{unit}.log"
        "logs/realigned/{sample}.log"
    conda:
        "../envs/bwa-samtools.yaml"
    threads: config['params']['gatk']['threads']
    shell: """
        java {params.jvm} -jar {params.jar} \
            -T IndelRealigner \
            -R {input.ref} \
            -I {input.bam} \
            -targetIntervals {input.intervals} \
            -o {output[0]} \
            2> {log}
        samtools index {output[0]}
        """
        
rule samtools_q0_depth:
    input:
        "data/realigned/{sample}.bam"
    output:
        "data/depths/{sample}_q0_depth.bed"
    conda:
        "../envs/bwa-samtools.yaml"
    shell: """
        samtools depth -a {input} > {output[0]}
    """

rule samtools_q20_depth:
    input:
        "data/realigned/{sample}.bam"
    output:
        "data/depths/{sample}_q20_depth.bed"
    conda:
        "../envs/bwa-samtools.yaml"
    params:
        MQ=config["params"]["depth"]["mq"]
    shell: """
        samtools depth -a -Q {params.MQ} {input} > {output[0]}
    """

rule freebayes_interval_MM:
    input:
        ref=config["genome"],
        bam=["data/realigned/{}.bam".format(x) for x in units.index.levels[0]],
        bai=["data/realigned/{}.bam.bai".format(x) for x in units.index.levels[0]],
        parent_bams=["../1_parent_snps/data/realigned/{}.bam".format(x) for x in config["parent_samples"]],
        parent_bais=["../1_parent_snps/data/realigned/{}.bam.bai".format(x) for x in config["parent_samples"]],
        dhpool_bams=["../4_dihaploid_pools/data/realigned/{}.bam".format(x) for x in config["dihaploid_pools"]],
        dhpool_bais=["../4_dihaploid_pools/data/realigned/{}.bam.bai".format(x) for x in config["dihaploid_pools"]]
    output:
        "data/calls/interval_MM/{interval}-calls.vcf"
    params:
        interval=lambda wildcards: re.sub(r"_(\d+)_", r":\1-", wildcards.interval),
        options=config["params"]["freebayes"]
    log:
        "log/freebayes/{interval}.log"
    conda:
        "../envs/freebayes.yaml"
    shell: """
        freebayes \
            -r {params.interval} \
            --fasta-reference {input.ref} \
            --bam {input.bam} {input.parent_bams} \
            --vcf {output} \
            {params.options} \
            2> {log}
    """

rule merge_MM_vcfs:
    input:
        vcfs=["data/calls/interval_MM/{}-calls.vcf".format(re.sub("\t", "_", x)) for x in intervals],
        intervals=config["intervals"]
    output:
        "data/calls/MM-calls.vcf"
    params:
        jarpath=config["params"]["mark_duplicates"]["jarpath"],
        java_heap=config["params"]["mark_duplicates"]["java_heap"]
    log:
        "logs/vcf_merge/MM_vcfs.log"
    conda:
        "../envs/picard.yaml"
    shell: """
        sed -e 's/^/data\/calls\/interval_MM\//g' \
            -e 's/$/-calls.vcf/g' {input.intervals} | \
        java {params.java_heap} -jar {params.jarpath} MergeVcfs \
            I=scaffold_calls.list \
            O={output} \
            2> {log}
    """

rule merge_MM_vcfs_to_chrom:
    input:
        vcfs=["data/calls/interval_MM/{}-calls.vcf".format(re.sub("\t", "_", x)) for x in intervals],
        intervals=config["intervals"]
    output:
        "data/calls/{chrom}-calls.vcf"
    params:
        jarpath=config["params"]["mark_duplicates"]["jarpath"],
        java_heap=config["params"]["mark_duplicates"]["java_heap"]
    log:
        "logs/vcf_merge/{chrom}_MM_vcfs.log"
    conda:
        "../envs/picard.yaml"
    shell: """
        sed -e 's/^/data\/calls\/interval_MM\//g' \
            -e 's/$/-calls.vcf/g' {input.intervals} | \
        grep {wildcards.chrom} | \
        tr "\t" "-" > {wildcards.chrom}_scaffold_calls.list
        java {params.java_heap} -jar {params.jarpath} MergeVcfs \
            I={wildcards.chrom}_scaffold_calls.list \
            O={output} \
            2> {log}
    """

rule vcf_site_filters:
    input:
        "data/calls/{chrom}-MM-calls.vcf"
    output:
        "data/calls/{chrom}-MM-filtered-calls.vcf.gz"
    shell: """
        Rscript scripts/2019_0225_filter_cheat_MM.R {input}
    """

rule intersect_offchrom:
    input:
        "../4_dihaploid_pools/data/snplists/{mom}-{pop}-hom-SNP.tsv",
        "data/calls/MM-calls.vcf"
    output:
        "data/parsed_calls/{mom}-{pop}-calls.vcf.gz"
    params:
    conda:
        "../envs/bedtools.yaml"
    shell: """
        cut -f 1-2 {input[0]} | \
        tail -n +2 | \
        awk -v OFS="\t" '{{print {{$1}},{{$2}}-1,{{$2}}}}' | \
        bedtools intersect -a {input[0]} -b stdin -sorted -header | gzip > {output}
    """